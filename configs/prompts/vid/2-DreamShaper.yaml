# Config File:

# Required Parameters:
input_video_path: "tmp/walk.mp4" # Path to the input video file
output_video_dir: "tmp/output"  # Directory to save the outputs

save_frames: True

# Width and Height of the Input and Output videos
# If zero the input's video dimension will be used otherwise the input will be resized
width: 640
height: 480

prompt: "1girl, one person, one face, highly detailed photo of (wonder woman)++ (perfect eyes)+ (perfect face)+ extremely sexy, full body, standing, posing, body, high detailed skin, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"
n_prompt: "3d, render, sketch, cartoon, drawing, anime, text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck"

# Additional Basic Parameters
start_time: "00:00:06" # Time in HH:MM:SS format to start reading the input video
end_time: "00:00:09" # Time in HH:MM:SS format to stop reading the input video

# Use img2img or not
use_img2img: False

# Base model that AnimateDiff uses to create the initial architecture from (it needs to be in HuggingFace format (.bin))
pretrained_model_path: "models/StableDiffusion/stable-diffusion-v1-5"

# Optional DreamBooth model (full)
dreambooth_path: "models/DreamBooth_LoRA/dreamshaper_8.safetensors" 

# Motion Module to be used - versions of the config and the model must match
inference_config_path: "configs/inference/inference-v2.yaml"
motion_module: "models/Motion_Module/mm_sd_v15_v2.ckpt"

# Optional ControlNet Models to be used - will be downloaded automatically
controlnets:
  - lllyasviel/sd-controlnet-openpose
  # - lllyasviel/sd-controlnet-canny
  # - lllyasviel/control_v11p_sd15_softedge
  # - lllyasviel/sd-controlnet-mlsd
  - lllyasviel/control_v11p_sd15_lineart

cond_scale: 
  - 1.0
  - 0.1

# Optional LoRA model to be used
lora_model_path: ""

# Advanced Parameters
frame_count: 16 # How many co-related frames are produced by AnimateDiff - defaults to 16

seed: -1 # 13100322578370451493 
steps: 60
guidance_scale: 9.5
strength: 0.75 # Strengtht of the noise to be added to input latents - if 1.0 the img2img effect is nil
overlap_length: 8 # Number of frames from previous output frames to be present in the current frames (helps with consistency)

fps: 12 # The framerate to sample the input video
fps_ffmpeg: 30 # The framerate of the output video
crf: 17 # A measure of quality - lower is better 

# The following controlnets are used to process the overlapping frames between AnimateDiff runs
# overlap_controlnets: 

# overlap_cond_scale:
#  - 1.0

# L: 16







