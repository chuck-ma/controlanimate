# Config File:

# Required Parameters:
input_video_path: "tmp/dance2.mp4" # Path to the input video file
output_video_dir: "tmp/output"  # Directory to save the outputs

save_frames: True

# Width and Height of the Input and Output videos
# If zero the input's video dimension will be used otherwise the input will be resized
width: 640
height: 480

prompt: "1girl, yoimiya+ (genshin impact)+, origen, line, comet, wink, Masterpiece, BestQuality, UltraDetailed"
n_prompt: "NSFW, lr, nsfw, sketch, duplicate, ugly, huge eyes, text, logo, monochrome, worst face, (bad and mutated hands)1.3, (worst quality)2.0, (low quality:2.0), (blurry:2.0), horror, geometry, bad_prompt_v2, (bad hands), (missing fingers), multiple limbs, bad anatomy, (interlocked fingers:1.2), Ugly Fingers, (extra digit and hands and fingers and legs and arms)1.4, crown braid, (2girl)++, (deformed fingers)1.2, (long fingers)1.2,succubus wings,horn,succubus horn,succubus hairstyle, (bad-artist-anime)+, bad-artist, bad hand, grayscale, skin spots, acnes, skin blemishes"

# Additional Basic Parameters
start_time: "00:00:00" # Time in HH:MM:SS format to start reading the input video
end_time: "00:00:04" # Time in HH:MM:SS format to stop reading the input video

# Use img2img or not
use_img2img: False

# Base model that AnimateDiff uses to create the initial architecture from (it needs to be in HuggingFace format (.bin))
pretrained_model_path: "models/StableDiffusion/stable-diffusion-v1-5"

# Optional DreamBooth model (full)
dreambooth_path: "models/DreamBooth_LoRA/AnythingV5_v5PrtRE.safetensors" 

# Optional LoRA model to be used
lora_model_path: models/DreamBooth_LoRA/genshin_impact_yoimiya.safetensors
lora_weight: 0.8

# Motion Module to be used - versions of the config and the model must match
inference_config_path: "configs/inference/inference-v2.yaml"
motion_module: "models/Motion_Module/mm_sd_v15_v2.ckpt"

# Optional ControlNet Models to be used - will be downloaded automatically
controlnets:
  - lllyasviel/sd-controlnet-openpose
  # - lllyasviel/sd-controlnet-canny
  # - lllyasviel/control_v11p_sd15_softedge
  # - lllyasviel/sd-controlnet-mlsd
  - lllyasviel/control_v11p_sd15_lineart

cond_scale: 
  - 1.5
  - 0.1

# Advanced Parameters
frame_count: 16 # How many co-related frames are produced by AnimateDiff - defaults to 16

seed: -1 # 13100322578370451493 
steps: 50
guidance_scale: 9.5
strength: 0.95 # Strengtht of the noise to be added to input latents - if 1.0 the img2img effect is nil
overlap_length: 8 # Number of frames from previous output frames to be present in the current frames (helps with consistency)

fps: 12 # The framerate to sample the input video
fps_ffmpeg: 30 # The framerate of the output video
crf: 17 # A measure of quality - lower is better 

# The following controlnets are used to process the overlapping frames between AnimateDiff runs
# overlap_controlnets: 

# overlap_cond_scale:
#  - 1.0

# L: 16







